{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can AI Detect Cancer?\n",
    "\n",
    "Let's look at a standard breast cancer dataset and practice evaluating a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "cancer = datasets.load_breast_cancer() # Load the cancer dataset from scikit-learn\n",
    "\n",
    "print(\"Feature Names\")\n",
    "print((cancer.feature_names)) \n",
    "\n",
    "# These are all the different features in the dataset. Would you consider dropping one of these features?\n",
    "print('')\n",
    "\n",
    "print(\"Names of Classes\")\n",
    "print((cancer.target_names))\n",
    "\n",
    "# We find that either a tumor is 'malignant' or it is 'benign'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot our data and see if we see any correlation between two features\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Mean radius')\n",
    "plt.ylabel('Mean texture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any clear data trends? Plotting helps machine learning researchers get a feel for what the data looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "Now, let's make a decision tree model which, given an arbitrary tumor, identifies whether it is malignant or benign.\n",
    "\n",
    "We want to figure out how well it is doing. We will use a classifier called a Decision Tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the training and testing datasets\n",
    "X = cancer.data # X, what we're using to predict, is the features\n",
    "y = cancer.target # y, what we're predicting, is the classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # We split the data into 'train' and 'test'\n",
    "\n",
    "# Train a classifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train) # Put these into the classifier\n",
    "\n",
    "# Use the trained classifier to predict whether a new piece of data is benign or malignant\n",
    "prediction = classifier.predict(X_test)\n",
    "\n",
    "# Note that these predictions aren't necessarily correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The accuracy, as we normally think of it, is defined as the number of correct predictions / total predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Correct and Total Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scratch way\n",
    "\n",
    "correct = [] # This list stores every prediction that was correct\n",
    "for i in range(0, len(y_test)): # Iterate through all of our predictions\n",
    "        if (y_test[i] == prediction[i]): # If our prediction matches the actual value,\n",
    "            correct.append(y_test[i]) # then put it in the 'correct' list\n",
    "\n",
    "num_correct = float(len(correct))\n",
    "num_pred = float(len(y_test))\n",
    "\n",
    "print(num_correct)\n",
    "print(num_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice\n",
    "\n",
    "Using the accuracy formula, define accuracy in terms of num_correct and num_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "accuracy = 0\n",
    "\n",
    "# Our accuracy is defined by the number of \n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A faster way is to do it with a built-in function, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\")\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "The formula for precision is (true positive)/(true positive + false positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Number of True Positives\n",
    "\n",
    "\n",
    "A true positive is a tumor prediction that says 'malignant' and matches with the correct value (in y_test).\n",
    "Note: 1 = 'malignant' and 0 = 'benign'\n",
    "To calculate the number of true positives, let's iterate through and find predictions that are both 'malignant' and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_positive = []\n",
    "for i in range(0, len(y_test)): # Iterate through all of our predictions\n",
    "        if (prediction[i] == 1 and y_test[i] == prediction[i]): # If we predict malignant and our prediction is correct\n",
    "            true_positive.append(y_test[i]) # then this prediction is a true positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Number of False Positives\n",
    "\n",
    "A false positive is a tumor prediction that says 'malignant' and does NOT match with the correct value (in y_test). To calculate the number of false positives, let's go through all of our predictions and see which ones are 'malignant' and don't match their correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive = []\n",
    "for i in range(0, len(y_test)): # Iterate through all of our predictions\n",
    "        if (prediction[i] == 1 and y_test[i] != prediction[i]): # If we predict malignant and our prediction is incorrect\n",
    "            false_positive.append(y_test[i]) # then this prediction is a false positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define num_true_pos as the length of the true positives list, and num_false_pos similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_true_pos = len(true_positive)\n",
    "num_false_pos = len(false_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice\n",
    "\n",
    "Your task is to now use num_true_pos and num_false_pos to calculate the precision, with our formula: precision = (true positive)/(true positive + false positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define variable 'precision' in terms of num_true_pos and num_false_pos, using our formula.\n",
    "precision = 0\n",
    "\n",
    "print(\"Precision: \")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "The formula for recall is (true positive)/(true positive + false negative).  \n",
    "We can use the true_positive array from earlier, so now all we have to calculate is a false_negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative = []\n",
    "for i in range(0, len(y_test)):\n",
    "    if (prediction[i] == 0 and y_test[i] != prediction[i]):\n",
    "        false_negative.append(y_test[i])\n",
    "\n",
    "num_false_neg = len(false_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice\n",
    "\n",
    "Your task is to define the 'recall' variable in terms of num_true_pos and num_false_neg. Remember that recall = (true positive)/(true positive + false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define variable 'recall' in terms of num_true_pos and num_false_neg.\n",
    "recall = 0\n",
    "print(\"Recall:\")\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Ethics Reflections\n",
    "\n",
    "1. Given that we're doing a cancer detection task, which error is worse: a false positive (reporting that a tumor is malignant when it actually isn't) or a false negative (reporting that a tumor is benign when it's not)?  \n",
    "\n",
    "\n",
    "2. Precision is best for when it's really bad to have false positives, and recall is the best for when it's bad to have false negatives. Given your answer from above and our precision or recall score, how do you think the model is doing? (5 = amazingly, 1 = very poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
